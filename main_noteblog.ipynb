{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone huggingface-hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import huggingface_hub\n",
    "\n",
    "audio_datasets_path = \".\"\n",
    "\n",
    "if not os.path.exists(audio_datasets_path):\n",
    "    os.makedirs(audio_datasets_path, exist_ok=True)\n",
    "\n",
    "huggingface_hub.snapshot_download(\n",
    "    repo_id=\"MahiA/GT-Music-Genre\", \n",
    "    repo_type=\"dataset\", \n",
    "    local_dir=os.path.join(audio_datasets_path, \"GT-Music-Genre\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "# Base directory\n",
    "base_dir = \"GT-Music-Genre\"\n",
    "\n",
    "# Create directory\n",
    "save_dir = os.path.join(base_dir, \"test\")\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Read the CSV file\n",
    "df = pd.read_csv(os.path.join(base_dir, \"test.csv\"))\n",
    "\n",
    "# Iterate through the DataFrame rows\n",
    "for _, row in df.iterrows():\n",
    "    # Get the source file path and genre\n",
    "    source_file = os.path.join(base_dir, row['path'])\n",
    "    genre = row['classname']\n",
    "    filename = os.path.basename(source_file)\n",
    "    \n",
    "    # Create genre subdirectory inside train directory if it doesn't exist\n",
    "    genre_dir = os.path.join(save_dir, genre)\n",
    "    os.makedirs(genre_dir, exist_ok=True)\n",
    "    \n",
    "    # Define destination path\n",
    "    dest_file = os.path.join(genre_dir, filename)\n",
    "    \n",
    "    # Move the file\n",
    "    if os.path.exists(source_file):\n",
    "        shutil.move(source_file, dest_file)\n",
    "    else:\n",
    "        print(f\"Warning: File not found - {source_file}\")\n",
    "\n",
    "print(\"Files have been organized into genre subdirectories within the directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['FIFTYONE_ALLOW_LEGACY_ORCHESTRATORS'] = 'true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's [download a plugin](https://github.com/danielgural/audio_loader/tree/main) that will create spectograms from the audio files.\n",
    "\n",
    "FiftyOne's plugin framework lets you extend and customize the functionality of FiftyOne to suit your needs. If youâ€™re interested in learning more about plugins, you might be interested in attending one of our monthly workshops. You can [see the full schedule here](https://voxel51.com/computer-vision-events/) and look for the *Advanced Computer Vision Data Curation and Model Evaluation* workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/danielgural/audio_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the plugin is downloaded there are two ways you can use it.\n",
    "\n",
    "1. You can launch the FiftyOne app in your local browser by opening the terminal and running: `fiftyone app launch`. Once the app has launched hit the backtick (\\`\\) button on your keyboard, this will open the Operator browser. Type in \"Load Audio\" and click on the operator. This will open up the form for the Load Audio plugin which you can fill in (each element of the form will appear once you populate each one). You can choose to kick off a [delegated service](https://docs.voxel51.com/plugins/developing_plugins.html#delegated-execution) if you'd like. \n",
    "\n",
    "Below is an example of the form:\n",
    "\n",
    "<img src=\"load_audio_form.png\" width=\"50%\"/>\n",
    "\n",
    "The plugin will take some moments to run, depending on the size of your dataset. In this case, it should take no more than 1 minute.\n",
    "\n",
    "2. Alternatively, instead of launching the app via terminal, you can launch the app in the cell of a Jupyter Notebook. To do that you must first create a dummy dataset and then launch the app in the cell. The pattern for this is as follows:\n",
    "\n",
    "```python\n",
    "import fiftyone as fo\n",
    "\n",
    "dummy_dataset = fo.Dataset()\n",
    "\n",
    "fo.launch_app(dummy_dataset)\n",
    "```\n",
    "Once the app has launched you can open the Operator browser and hit backtick (\\`\\), then follow the instructions as outlined above.\n",
    "\n",
    "In both cases, you can then load the dataset once it has been created. I named my dataset `music_genre_spectograms`, so I can load it as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "music_dataset = fo.load_dataset(\"music_genre_spectograms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(music_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need the labels, so we can get them like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_genres = music_dataset.distinct(\"ground_truth.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk about music2latent\n",
    "\n",
    "make mention that you should be on torch<2.6 and torchvision<0.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install music2latent librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "from music2latent import EncoderDecoder\n",
    "\n",
    "music_to_latent_model = EncoderDecoder()\n",
    "\n",
    "for sample in music_dataset.iter_samples(autosave=True):\n",
    "    wav_path = sample[\"wav_path\"]\n",
    "    sample_rate = sample[\"frame_rate\"]\n",
    "    loaded_wave, _ = librosa.load(wav_path, sr=sample_rate)\n",
    "    latents = music_to_latent_model.encode(loaded_wave, extract_features=True)\n",
    "    embedding = latents.mean(dim=-1).squeeze(0) \n",
    "    normalized_embedding = normalize(embedding, p=2, dim=0)\n",
    "    sample[\"wav_embedding\"] = normalized_embedding.detach().cpu().numpy() #shape (8192,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk real briefly about this model\n",
    "\n",
    "\n",
    "\n",
    "We'll use this model below for zero-shot-audio classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "import librosa\n",
    "\n",
    "from transformers import ClapModel, ClapProcessor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "clap_model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\").to(device)\n",
    "\n",
    "clap_processor = ClapProcessor.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "\n",
    "for sample in music_dataset.iter_samples(autosave=True):\n",
    "    wav_path = sample[\"wav_path\"]\n",
    "    sample_rate = sample[\"frame_rate\"]\n",
    "    loaded_wave, _ = librosa.load(wav_path, sr=sample_rate)\n",
    "    clap_inputs = clap_processor(audios=loaded_wave, return_tensors=\"pt\").to(device)\n",
    "    audio_embed = clap_model.get_audio_features(**clap_inputs).squeeze(0)  \n",
    "    normalized_embedding = normalize(audio_embed, p=2, dim=0)\n",
    "    sample[\"clap_embeddings\"] = normalized_embedding.detach().cpu().numpy() #shape (512,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll also compute embedding using AIMv2, which is a vision encoder. [Read this blog](https://medium.com/voxel51/visual-understanding-with-aimv2-76c58dcd68f9) for a deep dive into the AIMv2 family of models.\n",
    "\n",
    "This, dare I say, \"multimodal\" approach to analyzing embeddings provides different ways of exploring and understanding musical content, ultimately leading to an experiment with vision-language models (VLMs). Models like Music2Latent and CLAP operate directly on the raw audio waveforms, capturing temporal patterns, frequency relationships, and acoustic features in their native form. \n",
    "\n",
    "In parallel, we can compute embeddings using AIMv2 on the spectrograms - visual representations that encode time-frequency relationships in a 2D format.  This sets up (at least what I think is) a fascinating comparison: while the audio-specific models represent our 'traditional' approach to music understanding, the spectrogram-based analysis might hint at the suitability of a vision-language model to perform music classification. \n",
    "\n",
    "By converting audio into spectrograms, we can potentially tap into the sophisticated visual pattern recognition and semantic understanding capabilities of VLMs, even though they weren't specifically trained on musical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/harpreetsahota204/aim-embeddings-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "embedding_operator = foo.get_operator(\"@harpreetsahota/aimv2_embeddings/compute_aimv2_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_operator(\n",
    "    music_dataset,\n",
    "    model_name=\"apple/aimv2-large-patch14-224\",  # Choose any supported model\n",
    "    embedding_types=\"mean\",  # Either \"cls\" or \"mean\"\n",
    "    emb_field=\"aimv2_embeddings\",  # Name for the embeddings field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our embeddings to better understand how our different models are grouping similar music genres. \n",
    "\n",
    "Since our embeddings are high-dimensional, we'll use UMAP to reduce them to 2D for visualization. This will help us see if the models are clustering similar genres together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "embedding_fields = [ \"aimv2_embeddings\", \"wav_embedding\", \"clap_embeddings\"]\n",
    "\n",
    "for fields in embedding_fields:\n",
    "    _fname = fields.split(\"_embeddings\")[0]\n",
    "    results = fob.compute_visualization(\n",
    "        music_dataset,\n",
    "        embeddings=fields,\n",
    "        method=\"umap\",\n",
    "        brain_key=f\"{_fname}_viz\",\n",
    "        num_dims=2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(music_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before testing our VLM approach on spectrograms, we'll establish a baseline using a specialized audio model. \n",
    "\n",
    "We'll use LAION's CLAP model with a zero-shot audio classification pipeline. `This model was specifically trained on audio-text pairs and can classify audio into arbitrary categories without needing to be fine-tuned on our specific genre labels. `\n",
    "\n",
    "This will give us a reference point for how well a dedicated audio model performs on our genre classification task, which we can later compare against our VLM-based approach using spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "zsc_audio_classifier = pipeline(\n",
    "    task=\"zero-shot-audio-classification\", \n",
    "    model=\"laion/clap-htsat-unfused\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in music_dataset.iter_samples(autosave=True):\n",
    "    wav_path = sample[\"wav_path\"]\n",
    "    zsc_audio_preds = zsc_audio_classifier(wav_path, candidate_labels= music_genres)\n",
    "    sample[\"zsc_audio_preds\"] = fo.Classification(\n",
    "        label=zsc_audio_preds[0][\"label\"], \n",
    "        confidence=zsc_audio_preds[0][\"score\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation in FiftyOne\n",
    "\n",
    "You can use the [`evaluate_classifications`](https://docs.voxel51.com/tutorials/evaluate_classifications.html?highlight=evaluate%20classification) method to evaluate the predictions of the zero-shot classifiers. This will return a `ClassificationResults` instance that provides various methods for generating aggregate evaluation reports about your model.\n",
    "\n",
    "By default, the classifications will be treated as a generic multiclass classification task, and for illustration purposes, I am explicitly requesting that simple evaluation be used by setting the method argument to `simple`; but you can specify other evaluation strategies such as `top-k` accuracy or `binary` evaluation via the method parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "music_dataset.evaluate_classifications(\n",
    "    pred_field=\"zsc_audio_preds\",\n",
    "    gt_field=\"ground_truth\",\n",
    "    method=\"simple\",\n",
    "    eval_key=f\"clap_simple_eval\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moondream classification on spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Janus classification on spectogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
