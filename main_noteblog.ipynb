{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from datasets import load_dataset\n",
    "\n",
    "esc_fifty = load_dataset(\n",
    "    \"ashraq/esc50\", \n",
    "    split=\"train\",\n",
    "    cache_dir='.')\n",
    "\n",
    "def organize_esc10_dataset(dataset, base_output_dir=\"esc10_organized\"):\n",
    "    # Create base output directory\n",
    "    os.makedirs(base_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Filter for ESC-10 samples\n",
    "    esc10_samples = dataset.filter(lambda x: x['esc10'] == True)\n",
    "    \n",
    "    # Process each sample\n",
    "    for sample in esc10_samples:\n",
    "        category_dir = os.path.join(base_output_dir, sample['category'])\n",
    "        os.makedirs(category_dir, exist_ok=True)\n",
    "        \n",
    "        wav_path = os.path.join(category_dir, sample['filename'])\n",
    "        \n",
    "        # Convert float32 audio to int16 PCM\n",
    "        audio_array = sample['audio']['array']\n",
    "        # Normalize to [-1, 1] if not already\n",
    "        audio_array = audio_array / np.max(np.abs(audio_array))\n",
    "        # Convert to int16\n",
    "        audio_array = (audio_array * 32767).astype(np.int16)\n",
    "        \n",
    "        # Save audio array as wav file\n",
    "        wavfile.write(\n",
    "            wav_path, \n",
    "            sample['audio']['sampling_rate'],\n",
    "            audio_array\n",
    "        )\n",
    "    \n",
    "    print(f\"Dataset organized in {base_output_dir}\")\n",
    "    return esc10_samples\n",
    "\n",
    "organize_esc10_dataset(esc_fifty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's [download a plugin](https://github.com/danielgural/audio_loader/tree/main) that will create spectograms from the audio files.\n",
    "\n",
    "FiftyOne's plugin framework lets you extend and customize the functionality of FiftyOne to suit your needs. If you’re interested in learning more about plugins, you might be interested in attending one of our monthly workshops. You can [see the full schedule here](https://voxel51.com/computer-vision-events/) and look for the *Advanced Computer Vision Data Curation and Model Evaluation* workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/danielgural/audio_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the plugin is downloaded there are two ways you can use it.\n",
    "\n",
    "1. You can launch the FiftyOne app in your local browser by opening the terminal and running: `fiftyone app launch`. Once the app has launched hit the backtick (\\`\\) button on your keyboard, this will open the Operator browser. Type in \"Load Audio\" and click on the operator. This will open up the form for the Load Audio plugin which you can fill in (each element of the form will appear once you populate each one). You can choose to kick off a [delegated service](https://docs.voxel51.com/plugins/developing_plugins.html#delegated-execution) if you'd like. \n",
    "\n",
    "Below is an example of the form:\n",
    "\n",
    "<img src=\"load_audio_form.png\" width=\"50%\"/>\n",
    "\n",
    "The plugin will take some moments to run, depending on the size of your dataset. In this case, it should take no more than 1 minute.\n",
    "\n",
    "2. Alternatively, instead of launching the app via terminal, you can launch the app in the cell of a Jupyter Notebook. To do that you must first create a dummy dataset and then launch the app in the cell. The pattern for this is as follows:\n",
    "\n",
    "```python\n",
    "import fiftyone as fo\n",
    "\n",
    "dummy_dataset = fo.Dataset()\n",
    "\n",
    "fo.launch_app(dummy_dataset)\n",
    "```\n",
    "Once the app has launched you can open the Operator browser and hit backtick (\\`\\), then follow the instructions as outlined above.\n",
    "\n",
    "In both cases, you can then load the dataset once it has been created. Depending on what you named your dataset, you can load it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "audio_dataset = fo.load_dataset(\"esc10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s install a plugin that allows us to create custom dashboards and glean more insight into our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download \\\n",
    "    https://github.com/voxel51/fiftyone-plugins \\\n",
    "    --plugin-names @voxel51/dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(audio_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need the labels, so we can get them like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_classes = audio_dataset.distinct(\"ground_truth.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk about music2latent\n",
    "\n",
    "make mention that you should be on torch<2.6 and torchvision<0.21.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install music2latent librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "from music2latent import EncoderDecoder\n",
    "\n",
    "music_to_latent_model = EncoderDecoder()\n",
    "\n",
    "for sample in audio_dataset.iter_samples(autosave=True):\n",
    "    wav_path = sample[\"wav_path\"]\n",
    "    sample_rate = sample[\"frame_rate\"]\n",
    "    loaded_wave, _ = librosa.load(wav_path, sr=44100)\n",
    "    latents = music_to_latent_model.encode(loaded_wave, extract_features=True)\n",
    "    embedding = latents.mean(dim=-1).squeeze(0) \n",
    "    normalized_embedding = normalize(embedding, p=2, dim=0)\n",
    "    sample[\"wav_embedding\"] = normalized_embedding.detach().cpu().numpy() #shape (8192,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Talk real briefly about this model\n",
    "\n",
    "\n",
    "\n",
    "We'll use this model below for zero-shot-audio classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "import librosa\n",
    "\n",
    "from transformers import ClapModel, ClapProcessor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "clap_model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\").to(device)\n",
    "\n",
    "clap_processor = ClapProcessor.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "\n",
    "for sample in audio_dataset.iter_samples(autosave=True):\n",
    "    wav_path = sample[\"wav_path\"]\n",
    "    sample_rate = sample[\"frame_rate\"]\n",
    "    loaded_wave, _ = librosa.load(wav_path, sr=48000)\n",
    "    clap_inputs = clap_processor(audios=loaded_wave, return_tensors=\"pt\", sampling_rate=48000).to(device)\n",
    "    audio_embed = clap_model.get_audio_features(**clap_inputs).squeeze(0)  \n",
    "    normalized_embedding = normalize(audio_embed, p=2, dim=0)\n",
    "    sample[\"clap_embeddings\"] = normalized_embedding.detach().cpu().numpy() #shape (512,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll also compute embedding using AIMv2, which is a vision encoder. [Read this blog](https://medium.com/voxel51/visual-understanding-with-aimv2-76c58dcd68f9) for a deep dive into the AIMv2 family of models.\n",
    "\n",
    "This, dare I say, \"multimodal\" approach to analyzing embeddings provides different ways of exploring and understanding audio content, ultimately leading to an experiment with vision-language models (VLMs). Models like Music2Latent and CLAP operate directly on the raw audio waveforms, capturing temporal patterns, frequency relationships, and acoustic features in their native form. \n",
    "\n",
    "In parallel, we can compute embeddings using AIMv2 on the spectrograms - visual representations that encode time-frequency relationships in a 2D format.  This sets up (at least what I think is) a fascinating comparison: while the audio-specific models represent our 'traditional' approach to audio understanding, the spectrogram-based analysis might hint at the suitability of a vision-language model to perform audio classification. \n",
    "\n",
    "By converting audio into spectrograms, we can potentially tap into the sophisticated visual pattern recognition and semantic understanding capabilities of VLMs, even though they weren't specifically trained on audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/harpreetsahota204/aim-embeddings-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "embedding_operator = foo.get_operator(\"@harpreetsahota/aimv2_embeddings/compute_aimv2_embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_operator(\n",
    "    audio_dataset,\n",
    "    model_name=\"apple/aimv2-large-patch14-224\",  # Choose any supported model\n",
    "    embedding_types=\"mean\",  # Either \"cls\" or \"mean\"\n",
    "    emb_field=\"aimv2_embeddings\",  # Name for the embeddings field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our embeddings to better understand how our different models are grouping similar audio classes. \n",
    "\n",
    "Since our embeddings are high-dimensional, we'll use UMAP to reduce them to 2D for visualization. This will help us see if the models are clustering similar genres together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "embedding_fields = [ \"aimv2_embeddings\", \"wav_embedding\", \"clap_embeddings\"]\n",
    "\n",
    "for fields in embedding_fields:\n",
    "    _fname = fields.split(\"_embeddings\")[0]\n",
    "    results = fob.compute_visualization(\n",
    "        audio_dataset,\n",
    "        embeddings=fields,\n",
    "        method=\"umap\",\n",
    "        brain_key=f\"{_fname}_viz\",\n",
    "        num_dims=2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(audio_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before testing our VLM approach on spectrograms, we'll establish a baseline using a specialized audio model. \n",
    "\n",
    "We'll use LAION's CLAP model with a zero-shot audio classification pipeline. `This model was specifically trained on audio-text pairs and can classify audio into arbitrary categories without needing to be fine-tuned on our specific genre labels. `\n",
    "\n",
    "This will give us a reference point for how well a dedicated audio model performs on our genre classification task, which we can later compare against our VLM-based approach using spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "zsc_audio_classifier = pipeline(\n",
    "    task=\"zero-shot-audio-classification\", \n",
    "    model=\"laion/clap-htsat-unfused\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sample in audio_dataset.iter_samples(autosave=True):\n",
    "    wav_path = sample[\"wav_path\"]\n",
    "    zsc_audio_preds = zsc_audio_classifier(wav_path, candidate_labels= audio_classes)\n",
    "    sample[\"zsc_audio_preds\"] = fo.Classification(\n",
    "        label=zsc_audio_preds[0][\"label\"], \n",
    "        confidence=zsc_audio_preds[0][\"score\"]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation in FiftyOne\n",
    "\n",
    "You can use the [`evaluate_classifications`](https://docs.voxel51.com/tutorials/evaluate_classifications.html?highlight=evaluate%20classification) method to evaluate the predictions of the zero-shot classifiers. This will return a `ClassificationResults` instance that provides various methods for generating aggregate evaluation reports about your model.\n",
    "\n",
    "By default, the classifications will be treated as a generic multiclass classification task, and for illustration purposes, I am explicitly requesting that simple evaluation be used by setting the method argument to `simple`; but you can specify other evaluation strategies such as `top-k` accuracy or `binary` evaluation via the method parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dataset.evaluate_classifications(\n",
    "    pred_field=\"zsc_audio_preds\",\n",
    "    gt_field=\"ground_truth\",\n",
    "    method=\"simple\",\n",
    "    eval_key=f\"clap_simple_eval\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(audio_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VLMs\n",
    "\n",
    "Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_audio_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_audio_classes = ', '.join(audio_classes)\n",
    "\n",
    "vlm_query_prompt_long = f\"\"\"Your task is to analyze a spectrogram, which is a visual representation of the frequency spectrum \n",
    "of sound over time, and determine the most likely sound class from a given list of possibilities. \n",
    "\n",
    "Analyze the spectrogram image,considering factors such as frequency patterns, intensity, and time variations. \n",
    "\n",
    "Focus solely on the patterns presented in the spectrogram. Do not let any assumptions about common sounds or \n",
    "environmental settings influence your decision.  \n",
    "\n",
    "Here are the classes {string_audio_classes}:. \n",
    "\n",
    "Your response must always contain the exact name of the class only. \n",
    "\n",
    "For example, if you believe the spectrogram matches best with rain, your response would be rain. \n",
    "\n",
    "Here is the spectrogram:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vlm_query_prompt_short = f\"\"\"Which of the following classes {string_audio_classes} does this spectrogram best represent? \n",
    "Respond only with the name of the class, nothing more.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moondream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/harpreetsahota204/moondream2-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins requirements @harpreetsahota/moondream2 --install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "moondream = foo.get_operator(\"@harpreetsahota/moondream2/moondream\")\n",
    "\n",
    "await moondream(\n",
    "    audio_dataset,\n",
    "    revision=\"2025-01-09\",\n",
    "    operation=\"query\",\n",
    "    output_field=\"moondream_classification\",\n",
    "    query_text=vlm_query_prompt_short,\n",
    "    delegate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Janus classification on spectogram\n",
    "\n",
    "Janus-Pro is an advanced multimodal model designed for both multimodal understanding and visual generation, emphasizing improvements in understanding tasks. The model's architecture is built upon decoupled visual encoding, which allows it to handle the differing representation needs of these two types of tasks more effectively.\n",
    "\n",
    "NOTE: This plugin only supports multimodal understanding tasks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/harpreetsahota204/janus-vqa-fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins requirements @harpreetsahota/janus_vqa --install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "janus_vqa = foo.get_operator(\"@harpreetsahota/janus_vqa/janus_vqa\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "await janus_vqa(\n",
    "    audio_dataset,\n",
    "    model_path=\"deepseek-ai/Janus-Pro-1B\",\n",
    "    question=vlm_query_prompt_short,\n",
    "    question_field=\"query\",\n",
    "    answer_field=\"janus_classification\",\n",
    "    delegate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(audio_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
