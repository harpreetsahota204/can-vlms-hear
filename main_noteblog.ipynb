{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Can VLMs Hear What They See?\n",
    "\n",
    "I recently came across a paper that made me wonder: could we *actually* use vision language models to understand audio? \n",
    "\n",
    "The paper, [Vision Language Models Are Few-Shot Audio Spectrogram Classifiers](https://arxiv.org/abs/2411.12058),  introduces and explores Visual Spectrogram Classification (VSC), a task where visual language models (VLMs) classify audio by analyzing spectrograms (visual representations of sound). \n",
    "\n",
    "The key points are:\n",
    "\n",
    "1. Core concept: Converting audio classification into a visual task by having VLMs analyze spectrogram images\n",
    "\n",
    "2. Findings:\n",
    "- VLMs can effectively bridge visual-audio domains\n",
    "- Few-shot learning significantly improves performance\n",
    "- VLMs sometimes outperform both human experts and audio language models\n",
    "\n",
    "3. Significance:\n",
    "- Establishes a new benchmark for testing VLMs' audio understanding\n",
    "- Demonstrates potential for improving audio captioning\n",
    "- Shows promise in compensating for current limitations of audio language models\n",
    "\n",
    "\n",
    "This curiosity sent me down a day-long rabbit hole of experimentation, leading to some intriguing discoveries. While I won't be reproducing their few-shot learning approach, I wanted to explore whether Janus-Pro, a recently released VLM from DeepSeek AI, could tackle this task using zero-shot classification.\n",
    "\n",
    "The ESC-10 is an audio dataset for environmental sound classification. It contains a selection of 10 classes from [the larger ESC-50 dataset](https://github.com/karolpiczak/ESC-50). \n",
    "\n",
    "#### ESC-10 dataset.\n",
    "\n",
    "In the paper, the authors tested their hypothesis on the [ESC-10 dataset](https://github.com/karolpiczak/ESC-50). \n",
    "\n",
    "The ESC-10 dataset consists of 400 labeled environmental recordings divided into 10 classes with 40 clips per class, each lasting 5 seconds with a 44.1 kHz sampling rate. The dataset includes transient/percussive sounds and sounds with temporal patterns and the ten categories included are:\n",
    "\n",
    "- 🐕 Dog\n",
    "- 🐓 Rooster\n",
    "- 🌧️ Rain\n",
    "- 🌊 Sea waves\n",
    "- 🔥 Crackling\n",
    "- 👶 Crying baby\n",
    "- 🤧 Sneezing\n",
    "- ⏰ Clock tick\n",
    "- 🚁 Helicopter\n",
    "- 🪚 Chainsaw\n",
    "\n",
    "This code handles the organization and preprocessing of the ESC-10 dataset. Here's what's happening:\n",
    "\n",
    "1. We're loading the full ESC-50 dataset from HuggingFace using the `load_dataset` function\n",
    "2. The `organize_esc10_dataset` function then:\n",
    "   - Filters out only the ESC-10 samples (a subset of ESC-50)\n",
    "   - Creates a directory structure where each sound category has its own folder\n",
    "   - Processes each audio sample by:\n",
    "     - Normalizing the audio to the range [-1, 1]\n",
    "     - Converting from float32 to 16-bit PCM format (standard for WAV files)\n",
    "     - Saving each processed audio file to its respective category folder\n",
    "\n",
    "This organization:\n",
    "- Makes the dataset easier to work with\n",
    "- Ensures consistent audio format across all samples\n",
    "- Creates a clean directory structure for further processing\n",
    "- Prepares the audio files for spectrogram generation in later steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from datasets import load_dataset\n",
    "\n",
    "esc_fifty = load_dataset(\n",
    "    \"ashraq/esc50\", \n",
    "    split=\"train\",\n",
    "    cache_dir='.')\n",
    "\n",
    "def organize_esc10_dataset(dataset, base_output_dir=\"esc10_organized\"):\n",
    "    # Create base output directory\n",
    "    os.makedirs(base_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Filter for ESC-10 samples\n",
    "    esc10_samples = dataset.filter(lambda x: x['esc10'] == True)\n",
    "    \n",
    "    # Process each sample\n",
    "    for sample in esc10_samples:\n",
    "        category_dir = os.path.join(base_output_dir, sample['category'])\n",
    "        os.makedirs(category_dir, exist_ok=True)\n",
    "        \n",
    "        wav_path = os.path.join(category_dir, sample['filename'])\n",
    "        \n",
    "        # Convert float32 audio to int16 PCM\n",
    "        audio_array = sample['audio']['array']\n",
    "        # Normalize to [-1, 1] if not already\n",
    "        audio_array = audio_array / np.max(np.abs(audio_array))\n",
    "        # Convert to int16\n",
    "        audio_array = (audio_array * 32767).astype(np.int16)\n",
    "        \n",
    "        # Save audio array as wav file\n",
    "        wavfile.write(\n",
    "            wav_path, \n",
    "            sample['audio']['sampling_rate'],\n",
    "            audio_array\n",
    "        )\n",
    "    \n",
    "    print(f\"Dataset organized in {base_output_dir}\")\n",
    "\n",
    "organize_esc10_dataset(esc_fifty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's [download a plugin](https://github.com/danielgural/audio_loader/tree/main) that will create spectograms from the audio files.\n",
    "\n",
    "By converting audio into spectrograms, we can potentially tap into the sophisticated visual pattern recognition and semantic understanding capabilities of VLMs, even though they weren't specifically trained on audio data.\n",
    "\n",
    "> FiftyOne's plugin framework lets you extend and customize the functionality of FiftyOne to suit your needs. If you’re interested in learning more about plugins, you might be interested in attending one of our monthly workshops. You can [see the full schedule here](https://voxel51.com/computer-vision-events/) and look for the *Advanced Computer Vision Data Curation and Model Evaluation* workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/danielgural/audio_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the plugin is downloaded there are two ways you can use it.\n",
    "\n",
    "1. You can launch the FiftyOne app in your local browser by opening the terminal and running: `fiftyone app launch`. Once the app has launched hit the backtick (\\`\\) button on your keyboard, this will open the Operator browser. Type in \"Load Audio\" and click on the operator. This will open up the form for the Load Audio plugin which you can fill in (each element of the form will appear once you populate each one). You can choose to kick off a [delegated service](https://docs.voxel51.com/plugins/developing_plugins.html#delegated-execution) if you'd like. \n",
    "\n",
    "Below is an example of the form:\n",
    "\n",
    "<img src=\"assets/load_audio_form.png\" width=\"50%\"/>\n",
    "\n",
    "The plugin will take some moments to run, depending on the size of your dataset. In this case, it should take no more than 1 minute.\n",
    "\n",
    "2. Alternatively, instead of launching the app via terminal, you can launch the app in the cell of a Jupyter Notebook. To do that you must first create a dummy dataset and then launch the app in the cell. The pattern for this is as follows:\n",
    "\n",
    "```python\n",
    "import fiftyone as fo\n",
    "\n",
    "dummy_dataset = fo.Dataset()\n",
    "\n",
    "fo.launch_app(dummy_dataset)\n",
    "```\n",
    "Once the app has launched you can open the Operator browser and hit backtick (\\`\\), then follow the instructions as outlined above. In both cases, you can then load the dataset once it has been created. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Depending on what you named your dataset, you can load it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "audio_dataset = fo.load_dataset(\"esc10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s install a plugin that allows us to create custom dashboards and glean more insight into our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download \\\n",
    "    https://github.com/voxel51/fiftyone-plugins \\\n",
    "    --plugin-names @voxel51/dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(audio_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving deep into analysis, it's always good practice to do a quick \"vibe check\" of your dataset using FiftyOne's visualization capabilities. The app provides an intuitive interface to browse through your samples, inspect their metadata, and get a feel for the data distribution. \n",
    "\n",
    "You can:\n",
    "\n",
    "- Browse through spectrograms to check their quality and consistency\n",
    "- Filter and sort samples by different fields\n",
    "- Verify that labels are correctly assigned\n",
    "- Spot any obvious outliers or data quality issues\n",
    "- Get a sense of the class balance\n",
    "\n",
    "This visual inspection often reveals insights that might not be immediately apparent from looking at the raw data or statistics alone. \n",
    "\n",
    "<img src=\"assets/explore-esc10-spectrograms.gif\">\n",
    "\n",
    "We'll also need the labels later on, we can grab them like so:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_classes = audio_dataset.distinct(\"ground_truth.label\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's supplement our visual inspection by exploring how our audio samples relate to each other in high-dimensional space.  By visualizing embeddings, we can discover deeper patterns and relationships in our data:\n",
    "\n",
    "- Natural groupings and similarities between different sounds\n",
    "\n",
    "- Hidden structures that might not be obvious from spectrograms alone\n",
    "\n",
    "- Potential outliers or unusual samples in our dataset\n",
    "\n",
    "- Subtle acoustic patterns that connect different sound categories\n",
    "\n",
    "#### Computing embeddings\n",
    "\n",
    "I'll compute embeddings using three models:\n",
    "\n",
    "- Music2Latent\n",
    "\n",
    "- Clap\n",
    "\n",
    "- AIMv2\n",
    "\n",
    "This, dare I say, \"multimodal\" approach to analyzing embeddings provides different ways of exploring and understanding audio content, ultimately leading to an experiment with vision-language models (VLMs). Models like Music2Latent and CLAP operate directly on the raw audio waveforms, capturing temporal patterns, frequency relationships, and acoustic features in their native form. \n",
    "\n",
    "##### Music2Latent\n",
    "\n",
    "Music2Latent is an audio autoencoder that efficiently compresses audio into a smaller \"latent space\". It is designed for tasks like music generation and audio information retrieval.\n",
    "\n",
    "To extract audio features, the model encodes the audio using an encoder, and then the features from a specific layer are extracted and averaged. These features can then be used for various tasks. The model uses spectrograms (visual representations of audio frequencies) and consists of three main parts:\n",
    "\n",
    "*   **Encoder**: Compresses the audio into a latent vector.\n",
    "*   **Decoder**: Upsamples the latent vectors.\n",
    "*   **Consistency Model**: Reconstructs the audio from the latent vector.\n",
    "\n",
    "The extracted features can be used for tasks like auto-tagging, key estimation, and instrument/pitch classification, often outperforming other similar models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install music2latent librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "from music2latent import EncoderDecoder\n",
    "\n",
    "music_to_latent_model = EncoderDecoder()\n",
    "\n",
    "for sample in audio_dataset.iter_samples(autosave=True):\n",
    "    wav_path = sample[\"wav_path\"]\n",
    "    sample_rate = sample[\"frame_rate\"]\n",
    "    loaded_wave, _ = librosa.load(wav_path, sr=sample_rate)\n",
    "    latents = music_to_latent_model.encode(loaded_wave, extract_features=True)\n",
    "    embedding = latents.mean(dim=-1).squeeze(0) \n",
    "    normalized_embedding = normalize(embedding, p=2, dim=0)\n",
    "    sample[\"music2latent_embedding\"] = normalized_embedding.detach().cpu().numpy() #shape (8192,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CLAP\n",
    "\n",
    "The [CLAP model](https://arxiv.org/pdf/2211.06687) introduces a contrastive language-audio pretraining model designed for audio representation learning by combining audio data with natural language descriptions. \n",
    "\n",
    "The model can be used for:\n",
    "\n",
    "*   **Extracting Audio and Text Embeddings:** The model uses audio and text encoders to project audio and text data into a shared latent space, creating audio embeddings *Ea* and text embeddings *Et*. These embeddings can be used for various downstream tasks.\n",
    "\n",
    "*   **Zero-Shot Audio Classification:** The model can perform zero-shot audio classification by converting the classification task into a text-to-audio retrieval task. It matches a given audio *Xa* against a set of text prompts *Xt* (e.g., \"the sound of class-name\") and determines the best match based on cosine similarity between their embeddings. The categories of audio are unrestricted (i.e., zero-shot).\n",
    "\n",
    "We'll use this model later for zero-shot-audio classification, but for now let's compute embeddings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.functional import normalize\n",
    "\n",
    "import librosa\n",
    "\n",
    "from transformers import ClapModel, ClapProcessor\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "clap_model = ClapModel.from_pretrained(\"laion/clap-htsat-unfused\").to(device)\n",
    "\n",
    "clap_processor = ClapProcessor.from_pretrained(\"laion/clap-htsat-unfused\")\n",
    "\n",
    "for sample in audio_dataset.iter_samples(autosave=True):\n",
    "    wav_path = sample[\"wav_path\"]\n",
    "    loaded_wave, _ = librosa.load(wav_path, sr=48000)\n",
    "    clap_inputs = clap_processor(audios=loaded_wave, return_tensors=\"pt\", sampling_rate=48000).to(device)\n",
    "    audio_embed = clap_model.get_audio_features(**clap_inputs).squeeze(0)  \n",
    "    normalized_embedding = normalize(audio_embed, p=2, dim=0)\n",
    "    sample[\"clap_embeddings\"] = normalized_embedding.detach().cpu().numpy() #shape (512,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In parallel, we can compute embeddings using AIMv2 on the spectrograms - visual representations that encode time-frequency relationships in a 2D format.  \n",
    "\n",
    "This sets up (at least what I think is) a fascinating comparison: while the audio-specific models represent our 'traditional' approach to audio understanding, the spectrogram-based analysis might hint at the suitability of a vision-language model to perform audio classification. \n",
    "\n",
    " [Read this blog](https://medium.com/voxel51/visual-understanding-with-aimv2-76c58dcd68f9) for a deep dive into the AIMv2 family of models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/harpreetsahota204/aim-embeddings-plugin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "embedding_operator = foo.get_operator(\"@harpreetsahota/aimv2_embeddings/compute_aimv2_embeddings\")\n",
    "\n",
    "embedding_operator(\n",
    "    audio_dataset,\n",
    "    model_name=\"apple/aimv2-large-patch14-native\",\n",
    "    embedding_types=\"cls\",  # Either \"cls\" or \"mean\"\n",
    "    emb_field=\"aimv2_embeddings\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize our embeddings to better understand how our different models are grouping similar audio classes. \n",
    "\n",
    "Since our embeddings are high-dimensional, we'll use UMAP to reduce them to 2D for visualization. This will help us see if the models are clustering similar genres together.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.brain as fob\n",
    "\n",
    "embedding_fields = [ \"aimv2_embeddings\", \"music2latent_embedding\", \"clap_embeddings\"]\n",
    "\n",
    "for fields in embedding_fields:\n",
    "    _fname = fields.split(\"_embeddings\")[0]\n",
    "    results = fob.compute_visualization(\n",
    "        audio_dataset,\n",
    "        embeddings=fields,\n",
    "        method=\"umap\",\n",
    "        brain_key=f\"{_fname}_viz\",\n",
    "        num_dims=2,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(audio_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding Analysis\n",
    "\n",
    "Looking at the UMAP visualizations of the three embedding spaces reveals interesting patterns about how each model represents audio:\n",
    "\n",
    "1. CLAP embeddings show remarkably clear separation between sound categories, with distinct clusters for each class. This is expected given CLAP was specifically trained for audio understanding tasks.\n",
    "\n",
    "2. Music2Latent shows moderate clustering with some overlap between categories. The model appears to group similar acoustic properties together while maintaining some distinction between different sound types.\n",
    "\n",
    "3. AIMv2 embeddings, interestingly, show significant mixing between categories with no clear clustering pattern. Despite working with spectrograms, the vision model's embeddings don't appear to naturally separate different sound categories.\n",
    "\n",
    "\n",
    "<img src=\"assets/embeddings-esc10.gif\">\n",
    "\n",
    "\n",
    "#### Hypothesis for VLM Performance\n",
    "\n",
    "Given the significant overlap in AIMv2's embedding space, I might expect VLMs to face challenges when classifying spectrograms. The lack of natural clustering in the visual embedding space suggests that the spectrogram patterns might not map cleanly to sound categories from a pure vision perspective.\n",
    "\n",
    "I hypothesize that:\n",
    "\n",
    "1. The specialized audio model (CLAP) will significantly outperform the VLM approach\n",
    "\n",
    "2. VLMs might struggle with consistent classification across all categories\n",
    "\n",
    "3. The performance gap between CLAP and VLMs could highlight the limitations of treating audio classification as a pure visual task\n",
    "\n",
    "Let's test this hypothesis by implementing both approaches. \n",
    "\n",
    "#### Zero-shot audio classification\n",
    "\n",
    "I'll use LAION's CLAP (discussed above) model with a zero-shot audio classification pipeline. This will give us a reference point for how well a dedicated audio model performs on our genre classification task, which we can later compare against our VLM-based approach using spectrograms.\n",
    "\n",
    "The CLAP model was trained on several datasets. including:\n",
    "\n",
    "*   **AudioCaps+Clotho (AC+CL)**: This smaller dataset contains approximately 55,000 audio-text pairs.\n",
    "\n",
    "*   **LAION-Audio-630K (LA.)**: The LAION-Audio-630K dataset was newly created for this model and is the largest public audio caption dataset with 633,526 audio-text pairs.\n",
    "\n",
    "*   **AudioSet**: This dataset includes 1.9 million audio samples, originally with only labels, which were extended into captions using either a template or a keyword-to-caption model.\n",
    "\n",
    "The datasets were combined to increase the total number of audio samples with text captions to 2.5 million. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "zsc_audio_classifier = pipeline(\n",
    "    task=\"zero-shot-audio-classification\", \n",
    "    model=\"laion/clap-htsat-unfused\"\n",
    "    )\n",
    "\n",
    "for sample in audio_dataset.iter_samples(autosave=True):\n",
    "    wav_path = sample[\"wav_path\"]\n",
    "    zsc_audio_preds = zsc_audio_classifier(wav_path, candidate_labels= audio_classes)\n",
    "    sample[\"zsc_audio_preds\"] = fo.Classification(\n",
    "        label=zsc_audio_preds[0][\"label\"], \n",
    "        confidence=zsc_audio_preds[0][\"score\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model evaluation in FiftyOne\n",
    "\n",
    "You can use the [`evaluate_classifications`](https://docs.voxel51.com/tutorials/evaluate_classifications.html?highlight=evaluate%20classification) method to evaluate the predictions of the zero-shot classifiers. This will return a `ClassificationResults` instance that provides various methods for generating aggregate evaluation reports about your model.\n",
    "\n",
    "By default, the classifications will be treated as a generic multiclass classification task, and for illustration purposes, I am explicitly requesting that simple evaluation be used by setting the method argument to `simple`; but you can specify other evaluation strategies such as `top-k` accuracy or `binary` evaluation via the method parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zsc_results = audio_dataset.evaluate_classifications(\n",
    "    pred_field=\"zsc_audio_preds\",\n",
    "    gt_field=\"ground_truth\",\n",
    "    method=\"simple\",\n",
    "    eval_key=f\"clap_simple_eval\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(audio_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can evaluate model performance using the Model Evaluation panel in the app:\n",
    "\n",
    "<img src=\"assets/model-eval-esc10.gif\">\n",
    "\n",
    "\n",
    "Quite impressive performance, which I think will be hard to beat! You can also access the results programatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "      chainsaw       1.00      1.00      1.00        40\n",
      "    clock_tick       1.00      1.00      1.00        40\n",
      "crackling_fire       0.95      1.00      0.98        40\n",
      "   crying_baby       1.00      1.00      1.00        40\n",
      "           dog       1.00      1.00      1.00        40\n",
      "    helicopter       0.98      1.00      0.99        40\n",
      "          rain       1.00      0.95      0.97        40\n",
      "       rooster       1.00      1.00      1.00        40\n",
      "     sea_waves       1.00      0.97      0.99        40\n",
      "      sneezing       1.00      1.00      1.00        40\n",
      "\n",
      "      accuracy                           0.99       400\n",
      "     macro avg       0.99      0.99      0.99       400\n",
      "  weighted avg       0.99      0.99      0.99       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "zsc_results.print_report()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy   0.9925\n",
      "precision  0.9928\n",
      "recall     0.9925\n",
      "fscore     0.9925\n",
      "support    400\n"
     ]
    }
   ],
   "source": [
    "zsc_results.print_metrics(average=\"macro\", digits=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zero-shot spectrogram classification with Janus Pro\n",
    "\n",
    "Janus-Pro is an advanced multimodal model designed for both multimodal understanding and visual generation, emphasizing improvements in understanding tasks. The model's architecture is built upon decoupled visual encoding, which allows it to handle the differing representation needs of these two types of tasks more effectively.\n",
    "\n",
    "I've developed [a plugin for Janus Pro](https://github.com/harpreetsahota204/janus-vqa-fiftyone) that allows you to easily run the model on your FiftyOne dataset.\n",
    "\n",
    "Start by downloading the plugin, installing the requirements, and instantiating the operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins download https://github.com/harpreetsahota204/janus-vqa-fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fiftyone plugins requirements @harpreetsahota/janus_vqa --install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone.operators as foo\n",
    "\n",
    "janus_vqa = foo.get_operator(\"@harpreetsahota/janus_vqa/janus_vqa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the *Vision Language Models Are Few-Shot Audio\n",
    "Spectrogram Classifiers* paper the authors use the following prompt in the zero shot setting\n",
    "\n",
    "<img src=\"assets/figure5.png\" width=\"60%\">\n",
    "\n",
    "However, in that paper, the autors experimented with large models such as GPT-4o, Claude-3.5 Sonnet, and Gemini-1.5. Since we're working with a smaller model, I'll construct a more concise prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_audio_classes = ', '.join(audio_classes)\n",
    "\n",
    "vlm_query_prompt = f\"\"\"This is an image of a spectrogram. Which of the following classes does this spectrogram best represent: [{string_audio_classes}]\n",
    "Your response should be one word, the name of the class and nothing else.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running this operator we'll need to kickoff a delegated service. You can do this by running `fiftyone delegated launch` in your terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fiftyone.operators.executor.ExecutionResult at 0x75c8beb1aed0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "await janus_vqa(\n",
    "    audio_dataset,\n",
    "    model_path=\"deepseek-ai/Janus-Pro-7B\", #or you could pass deepseek-ai/Janus-Pro-1B\n",
    "    question=vlm_query_prompt,\n",
    "    question_field=\"query\",\n",
    "    answer_field=\"janus_classification\",\n",
    "    delegate=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default the plugin outputs it's result as a FiftyOne StringField, but we'll need to have them parsed as FiftyOne Classifications so that we can use `evaluate_classifications`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications = [fo.Classification(label=cls) for cls in audio_dataset.values(\"janus_classification\")]\n",
    "\n",
    "audio_dataset.set_values(\"janus_as_classification\", classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "janus_results = audio_dataset.evaluate_classifications(\n",
    "    pred_field=\"janus_as_classification\",\n",
    "    gt_field=\"ground_truth\",\n",
    "    method=\"simple\",\n",
    "    eval_key=f\"janus_simple_eval\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get an idea of model performance right off the bat:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'rain': 28, 'clock_tick': 1, 'dog': 355, 'crying_baby': 16}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_dataset.count_values(\"janus_classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, on first glance the results don't look promising at all! Let's dig a bit deeper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "      chainsaw       0.00      0.00      0.00        40\n",
      "    clock_tick       1.00      0.03      0.05        40\n",
      "crackling_fire       0.00      0.00      0.00        40\n",
      "   crying_baby       0.00      0.00      0.00        40\n",
      "           dog       0.11      0.95      0.19        40\n",
      "    helicopter       0.00      0.00      0.00        40\n",
      "          rain       0.11      0.07      0.09        40\n",
      "       rooster       0.00      0.00      0.00        40\n",
      "     sea_waves       0.00      0.00      0.00        40\n",
      "      sneezing       0.00      0.00      0.00        40\n",
      "\n",
      "      accuracy                           0.10       400\n",
      "     macro avg       0.12      0.10      0.03       400\n",
      "  weighted avg       0.12      0.10      0.03       400\n",
      "\n"
     ]
    }
   ],
   "source": [
    "janus_results.print_report()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the model evaluation panel to compare model performance.\n",
    "\n",
    "<img src=\"assets/model-comparison.gif\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fo.launch_app(audio_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lackluster performance of Janus-Pro on this zero-shot classification task isn't entirely surprising for several reasons:\n",
    "\n",
    "1. Model Size and Training: Unlike the larger models used in the original paper (GPT-4V, Claude-3, Gemini-1.5), Janus-Pro is a significantly smaller model. This likely limits its ability to make nuanced distinctions between spectrogram patterns.\n",
    "\n",
    "2. Prompt Engineering: The simplified prompt we used, while necessary for the smaller model, might not provide enough context about how to interpret spectrograms. A more detailed prompt explaining the time-frequency relationships in spectrograms could potentially improve performance.\n",
    "\n",
    "3. Zero-Shot vs Few-Shot: The original paper demonstrated that few-shot learning significantly improved performance. By showing the model examples of each class, it can better learn the visual patterns associated with different sounds. Our zero-shot approach, while simpler, leaves the model to figure out these patterns from scratch.\n",
    "\n",
    "4. Visual Embedding Analysis: Looking back at our AIMv2 embedding visualization, the significant overlap between categories in the visual space suggested that pure vision-based approaches might struggle with this task. The CLAP embeddings, which showed clear clustering, reinforce that audio-specific architectures might be better suited for this task.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "While Janus-Pro's performance on zero-shot audio classification yield impressive results, this exploration yielded valuable insights into the intersection of vision and audio understanding:\n",
    "\n",
    "1. **Embedding Analysis Revelations**: Our comparison of different embedding spaces (CLAP, Music2Latent, and AIMv2) provided fascinating insights into how different models interpret audio data. The clear clustering in CLAP's embeddings versus the mixed representations in AIMv2's space highlighted the importance of domain-specific architectures.\n",
    "\n",
    "2. **Baseline Performance**: CLAP's strong zero-shot performance established a compelling baseline, demonstrating the current capabilities of dedicated audio understanding models. This gives us a clear reference point for evaluating future multimodal approaches.\n",
    "\n",
    "3. **VLM Limitations and Potential**: While Janus-Pro struggled with zero-shot classification, this experiment helps us understand the current limitations of treating audio classification as a pure visual task. It also suggests that with few-shot learning, larger models, and better prompt engineering, VLMs might still have untapped potential in audio understanding.\n",
    "\n",
    "This journey from paper to practice not only satisfied my initial curiosity but also provided hands-on experience with cutting-edge multimodal models. Even when experiments don't yield the results we hope for, they often teach us the most valuable lessons about model capabilities, limitations, and the exciting challenges that lie ahead in bridging different modalities of perception."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fiftyone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
